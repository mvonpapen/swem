{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how to use the package for text classification tasks. We will introduce some of the functionality along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by producing a dummy dataset to train our model on. The dataset is constructed as follows:\n",
    "\n",
    "- The dataset has three possible labels \"A\", \"B\" and \"C\".\n",
    "- Each sample is a short 'text' of length between 5 and 25 tokens over the vocabulary [\"a\", \"b\", \"c\", \"d\", \"e\"].\n",
    "- A text with label \"A\" can contain any token other than \"a\" and analogous for \"B\" and \"C\".\n",
    "- For simplicities sake we pad every text to length 25 with the padding token \"p\" so that the full vocabulary is [\"p\", \"a\", \"b\", \"c\", \"d\", \"e\"].\n",
    "- We sample 3000 texts of which we will use the first 2500 for training and the last 500 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"A\", \"B\", \"C\"]\n",
    "vocab_nopad = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "vocab = [\"p\"] + vocab_nopad\n",
    "data = [\n",
    "    {\n",
    "        \"label\": (label:=random.choice(target_names)),\n",
    "        \"tokens\": random.choices([t for t in vocab_nopad if label.lower() != t], k=(k:=random.randint(5,25))) + [\"p\"] * (25-k)\n",
    "    }\n",
    "    for _ in range(3000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'C', 'tokens': ['a', 'e', 'b', 'e', 'b', 'b', 'a', 'd', 'a', 'd', 'a', 'a', 'e', 'd', 'e', 'a', 'b', 'd', 'b', 'd', 'b', 'p', 'p', 'p', 'p']}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to turn our data into tensors that our model can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {t: i for i,t in enumerate(vocab)}\n",
    "label2idx = {l: i for i,l in enumerate(target_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tensor = torch.tensor([[token2idx[t] for t in sample[\"tokens\"]] for sample in data], dtype=torch.int64)\n",
    "label_tensor = torch.tensor([label2idx[sample[\"label\"]]  for sample in data], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of use we wrap everything in a pytorch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = torch.utils.data.TensorDataset(text_tensor[:2500], label_tensor[:2500])\n",
    "testdata = torch.utils.data.TensorDataset(text_tensor[2500:], label_tensor[2500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a small SWEM-model to train on the data. The model will have the following structure:\n",
    "\n",
    "- A worddrop embedding with 6 embeddings (corresponding to the vocab) and an embedding dimension of 3.\n",
    "- A linear layer of size 3.\n",
    "- A hierarchical pooling layer with window size 4.\n",
    "- Another linear layer of size 3 and a final layer of size 3 whose outputs are the class logits for the classification task.\n",
    "\n",
    "We could construct the model directly from its *\\_\\_init\\_\\_*-method but we can also specify the configuration first and let the *from_config*-method do the rest for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swem.models.swem import SwemConfig, Swem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SwemConfig.from_dict({\n",
    "    \"embedding\": {\n",
    "        \"type\": \"WordDropEmbedding\",\n",
    "        \"num_embeddings\": 6,\n",
    "        \"embedding_dim\": 3,\n",
    "        \"padding_idx\": 0,\n",
    "        \"p\": 0.2\n",
    "    },\n",
    "    \"pooling\": {\n",
    "        \"type\": \"HierarchicalPooling\",\n",
    "        \"window_size\": 4\n",
    "    },\n",
    "    \"pre_pooling_dims\": (3, ),\n",
    "    \"post_pooling_dims\": (3, 3),\n",
    "    \"dropout\": 0.2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Swem.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Swem(\n",
       "  (embedding): WordDropEmbedding(6, 3, padding_idx=0)\n",
       "  (pooling_layer): HierarchicalPooling(\n",
       "    (avg_pooling): AvgPool2d(kernel_size=(4, 1), stride=1, padding=0)\n",
       "  )\n",
       "  (pre_pooling_trafo): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (post_pooling_trafo): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=3, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model for 20 epochs with a batch size of 8 and an Adam optimizer with learning rate 3e-4 (Karpathy's constant ðŸ˜‰). If a GPU is available we use it otherwise we default to the CPU. We use the *to_device*-function to transfer the tokens and labels to the specified device at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swem.utils.torch_utils import to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 8\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Starting epoch 2\n",
      "Starting epoch 3\n",
      "Starting epoch 4\n",
      "Starting epoch 5\n",
      "Starting epoch 6\n",
      "Starting epoch 7\n",
      "Starting epoch 8\n",
      "Starting epoch 9\n",
      "Starting epoch 10\n",
      "Starting epoch 11\n",
      "Starting epoch 12\n",
      "Starting epoch 13\n",
      "Starting epoch 14\n",
      "Starting epoch 15\n",
      "Starting epoch 16\n",
      "Starting epoch 17\n",
      "Starting epoch 18\n",
      "Starting epoch 19\n",
      "Starting epoch 20\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    traindata,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Starting epoch {i+1}\")\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = to_device(batch, device=device)\n",
    "        tokens, labels = batch\n",
    "        output = model(tokens)\n",
    "        loss = loss_fn(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate our model on the testset. For that purpose we iterate batchwise over the testset and aggregate the metrics along the way. This is achieved by making use of the *ClassificationReport*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swem.metrics import ClassificationReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ClassificationReport(target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    testdata,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        batch = to_device(batch, device=device)\n",
    "        tokens, labels = batch\n",
    "        logits = model(tokens)\n",
    "        report.update(logits, labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"num_samples\": 500,\n",
       "  \"accuracy\": 0.644,\n",
       "  \"class_metrics\": {\n",
       "    \"A\": {\n",
       "      \"support\": 168,\n",
       "      \"recall\": 1.0,\n",
       "      \"precision\": 0.9230769230769231,\n",
       "      \"f1_score\": 0.9600000000000001\n",
       "    },\n",
       "    \"B\": {\n",
       "      \"support\": 160,\n",
       "      \"recall\": 0.9375,\n",
       "      \"precision\": 0.4838709677419355,\n",
       "      \"f1_score\": 0.6382978723404255\n",
       "    },\n",
       "    \"C\": {\n",
       "      \"support\": 172,\n",
       "      \"recall\": 0.023255813953488372,\n",
       "      \"precision\": 0.5,\n",
       "      \"f1_score\": 0.04444444444444444\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save our model to disk for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *.save*-method save both the config and the weights in the directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('model/weights.pt'), PosixPath('model/config.json')]\n"
     ]
    }
   ],
   "source": [
    "print(list(model_path.iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use the model later on we can simply load it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = Swem.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure config and weights of the loaded model are actually the same as for the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.config == model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(\n",
    "    torch.allclose(model_param, model_loaded.state_dict()[param_name]) \n",
    "    for param_name, model_param in model.state_dict().items()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:swem]",
   "language": "python",
   "name": "conda-env-swem-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
