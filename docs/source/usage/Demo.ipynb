{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how to use the package for text classification tasks. We will introduce some of the functionality along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by producing a dummy dataset to train our model on. The dataset is constructed as follows:\n",
    "- The dataset has three possible labels \"A\", \"B\" and \"C\".\n",
    "- Each sample is a short 'text' of length between 5 and 25 tokens over the vocabulary [\"a\", \"b\", \"c\", \"d\", \"e\"].\n",
    "- A text with label \"A\" can contain any token other than \"a\" and analogous for \"B\" and \"C\".\n",
    "- For simplicities sake we pad every text to length 25 with the padding token \"p\" so that the full vocabulary is [\"p\", \"a\", \"b\", \"c\", \"d\", \"e\"].\n",
    "- We sample 3000 texts of which we will use the first 2500 for training and the last 500 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"A\", \"B\", \"C\"]\n",
    "vocab_nopad = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "vocab = [\"p\"] + vocab_nopad\n",
    "data = [\n",
    "    {\n",
    "        \"label\": (label:=random.choice(target_names)),\n",
    "        \"tokens\": random.choices([t for t in vocab_nopad if label.lower() != t], k=(k:=random.randint(5,25))) + [\"p\"] * (25-k)\n",
    "    }\n",
    "    for _ in range(3000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'B', 'tokens': ['e', 'd', 'a', 'a', 'a', 'c', 'a', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p']}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to turn our data into tensors that our model can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {t: i for i,t in enumerate(vocab)}\n",
    "label2idx = {l: i for i,l in enumerate(target_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tensor = torch.tensor([[token2idx[t] for t in sample[\"tokens\"]] for sample in data], dtype=torch.int64)\n",
    "label_tensor = torch.tensor([label2idx[sample[\"label\"]]  for sample in data], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of use we wrap everything in a pytorchd dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = torch.utils.data.TensorDataset(text_tensor[:2500], label_tensor[:2500])\n",
    "testdata = torch.utils.data.TensorDataset(text_tensor[2500:], label_tensor[2500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a small SWEM-model to train on the data. The model will have the following structure:\n",
    "- A worddrop embedding with 6 embeddings (corresponding to the vocab) and an embedding dimension of 3.\n",
    "- A linear layer of size 3.\n",
    "- A hierarchical pooling layer with window size 4.\n",
    "- Another linear layer of size 3 and a final layer of size 3 whose outputs are the class logits for the classification task.\n",
    "\n",
    "We could construct the model directly from its *\\_\\_init\\_\\_*-method but we can also specify the configuration first and let the *from_config*-method to the rest for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swem.models.swem import SwemConfig, Swem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SwemConfig.from_dict({\n",
    "    \"embedding\": {\n",
    "        \"type\": \"WordDropEmbedding\",\n",
    "        \"num_embeddings\": 6,\n",
    "        \"embedding_dim\": 3,\n",
    "        \"padding_idx\": 0,\n",
    "        \"p\": 0.2\n",
    "    },\n",
    "    \"pooling\": {\n",
    "        \"type\": \"HierarchicalPooling\",\n",
    "        \"window_size\": 4\n",
    "    },\n",
    "    \"pre_pooling_dims\": (3, ),\n",
    "    \"post_pooling_dims\": (3, 3),\n",
    "    \"dropout\": 0.2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Swem.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Swem(\n",
       "  (embedding): WordDropEmbedding(6, 3, padding_idx=0)\n",
       "  (pooling_layer): HierarchicalPooling(\n",
       "    (avg_pooling): AvgPool2d(kernel_size=(4, 1), stride=1, padding=0)\n",
       "  )\n",
       "  (pre_pooling_trafo): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (post_pooling_trafo): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=3, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model for 10 epochs with a batch size of 8 and an Adam optimizer with learning rate 3e-4 (Karpathy's constant ðŸ˜‰). If a GPU is available we use it otherwise we default to the CPU. We use the *to_device*-function to transfer the tokens and labels to the specified device at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swem.utils.torch_utils import to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 8\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Starting epoch 2\n",
      "Starting epoch 3\n",
      "Starting epoch 4\n",
      "Starting epoch 5\n",
      "Starting epoch 6\n",
      "Starting epoch 7\n",
      "Starting epoch 8\n",
      "Starting epoch 9\n",
      "Starting epoch 10\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    traindata,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Starting epoch {i+1}\")\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = to_device(batch, device=device)\n",
    "        tokens, labels = batch\n",
    "        output = model(tokens)\n",
    "        loss = loss_fn(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate our model on the testset. For that purpose we iterate batchwise over the testset and aggregate the metrics along the way. This is achieved by making use of the *ClassificationReport*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swem.metrics import ClassificationReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ClassificationReport(target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    testdata,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        batch = to_device(batch, device=device)\n",
    "        tokens, labels = batch\n",
    "        logits = model(tokens)\n",
    "        report.update(logits, labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"num_samples\": 500,\n",
       "  \"accuracy\": 0.96,\n",
       "  \"class_metrics\": {\n",
       "    \"A\": {\n",
       "      \"support\": 165,\n",
       "      \"recall\": 0.9636363636363636,\n",
       "      \"precision\": 0.9520958083832335,\n",
       "      \"f1_score\": 0.9578313253012049\n",
       "    },\n",
       "    \"B\": {\n",
       "      \"support\": 174,\n",
       "      \"recall\": 1.0,\n",
       "      \"precision\": 0.9354838709677419,\n",
       "      \"f1_score\": 0.9666666666666666\n",
       "    },\n",
       "    \"C\": {\n",
       "      \"support\": 161,\n",
       "      \"recall\": 0.9130434782608695,\n",
       "      \"precision\": 1.0,\n",
       "      \"f1_score\": 0.9545454545454545\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:swem]",
   "language": "python",
   "name": "conda-env-swem-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
